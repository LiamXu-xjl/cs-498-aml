{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from utils import test_case_checker, perform_computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Assignment Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exercise from the textbook (8.3.2. Example: Activity from Accelerometer Data):\n",
    "\n",
    "Obtain the activities of daily life dataset from the UC Irvine machine learning website (https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer, data provided by Barbara Bruno, Fulvio Mastrogiovanni and Antonio Sgorbissa).\n",
    "  1. Build a classifier that classifies sequences into one of the 14 activities provided. To make features, you should vector quantize, then use a histogram of cluster centers found through hierarchical k-means. For classification, any multi-class classifier works, but in this assignment we will use a decision forest because it is easy to use and effective. You should report (a) the total error rate and (b) the class confusion matrix of your classifier.\n",
    "  2. Now see if you can improve your classifier by (a) modifying the number of cluster centers in your hierarchical k-means and (b) modifying the size of the fixed length samples that you use.\n",
    "\n",
    "Questions about the homework:\n",
    "\n",
    "  1. How should we handle test/train splits?\n",
    "  \n",
    "  **Answer**: You should not test on examples that you used to build the dictionary, but you can train on them. In a perfect world, I would split the volunteers into a dictionary portion (about half), then do a test/train split for the classifier on the remaining half. You can't do that, because for some signals there are very few volunteers. For each category, choose 20% of the signals (or close!) for testing. Then use the others to both build the dictionary and build the classifier.\n",
    "\n",
    "  2. When we carve up the signals into blocks for making the dictionary, what do we do about leftover bits at the end of the signal?\n",
    "  \n",
    "  **Answer**: Ignore them; they shouldn't matter (think through the logic of the method again if you're uncertain about this)\n",
    "  \n",
    "\n",
    "<font color='red'> <b>Attention:</b> </font> After finishing this notebook, you will need to do a follow-up quiz as well. The overall grade for this asiggnment is based on this notebook and the follow-up quiz.\n",
    "    \n",
    "<font color='red'><b> Warning: </b></font> Using the \"Validate\" button for this assignment may lead to a timeout, please do not use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the activities of daily life dataset from the UC Irvine machine learning website (https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer).\n",
    "The data was provided by Barbara Bruno, Fulvio Mastrogiovanni and Antonio Sgorbissa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Information Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Input/Output**: The data includes 14 directories, each of which represents a certain daily activity. There are 839 accelerometer recordings in the dataset, each with 3 columns and some number of rows. The sampling frequency of the device was 32 samples per second.\n",
    "\n",
    "* **Missing Data**: There is no missing data. However, the data is not very well balanced, and some categories have really small amounts of data.\n",
    "\n",
    "* **Final Goal**: We want to build a classifier using vector quantization and other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the data\n",
    "with ZipFile('hmpdata.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into lists of lists\n",
    "col_labels = ['X','Y','Z']\n",
    "raw_txt_files = []\n",
    "activity_labels = ['Liedown_bed', 'Walk', 'Eat_soup', 'Getup_bed', 'Descend_stairs', \n",
    "                   'Use_telephone', 'Standup_chair', 'Brush_teeth', 'Climb_stairs', \n",
    "                   'Sitdown_chair', 'Eat_meat', 'Comb_hair', 'Drink_glass', 'Pour_water']\n",
    "\n",
    "for activity in activity_labels:\n",
    "    activity_txts = []\n",
    "    for file in os.listdir('HMP_Dataset/'+activity):\n",
    "        txtdf = pd.read_csv('HMP_Dataset/'+activity+'/'+file, names=col_labels,  sep=\" \")\n",
    "        activity_txts.append(txtdf)\n",
    "    raw_txt_files.append(activity_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up after we're done\n",
    "shutil.rmtree('./HMP_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each activity:\n",
      "    Liedown_bed: 28\n",
      "    Walk: 100\n",
      "    Eat_soup: 3\n",
      "    Getup_bed: 101\n",
      "    Descend_stairs: 42\n",
      "    Use_telephone: 13\n",
      "    Standup_chair: 102\n",
      "    Brush_teeth: 12\n",
      "    Climb_stairs: 102\n",
      "    Sitdown_chair: 100\n",
      "    Eat_meat: 5\n",
      "    Comb_hair: 31\n",
      "    Drink_glass: 100\n",
      "    Pour_water: 100\n",
      "Total number of samples: 839\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples for each activity:')\n",
    "for activity, activity_txts in zip(activity_labels, raw_txt_files):\n",
    "    print(f'    {activity}: {len(activity_txts)}')\n",
    "print(f'Total number of samples: {sum(len(activity_txts) for activity_txts in raw_txt_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Creating a Random Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not wise to out-source this train-test split to traditional sklearn functions as the data is a bit unique (not in a data matrix format), and also balancing the data in the small sample classes requires some delicacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_portion = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_random = np.random.RandomState(12345)\n",
    "train_val_txt_files = []\n",
    "test_txt_files = []\n",
    "for _,activity_txt_files in enumerate(raw_txt_files):\n",
    "    num_txt_files = len(activity_txt_files)\n",
    "    shuffled_indices = np.arange(num_txt_files)\n",
    "    np_random.shuffle(shuffled_indices)\n",
    "    \n",
    "    train_val_txt_files.append([])\n",
    "    test_txt_files.append([])\n",
    "    for i, idx in enumerate(shuffled_indices):\n",
    "        if i < test_portion * num_txt_files:\n",
    "            test_txt_files[-1].append(activity_txt_files[idx])\n",
    "        else:\n",
    "            train_val_txt_files[-1].append(activity_txt_files[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll assume the following two hyper-parameters:\n",
    "1. `d`: This is the vector quantization length in the number of rows. This default value of 32 corresponds to about 1 full second of observation.\n",
    "2. `k`: This is the number of K-Means clusters for creating features using cluster histograms.\n",
    "\n",
    "Since we do not want to engage in any hyper-parameter tuning yet, we will use the whole `train_val_txt_files` data for just training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 32\n",
    "k = 100\n",
    "train_txt_files = train_val_txt_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a vector-quantization function `quantize` that takes two arguments as input\n",
    "\n",
    "1. `X`: a numpy array with the shape $(N, 3)$, where $N$ is the number of samples in a single recording. The columns represent the acceleration in each of the $x$, $y$, and $z$ directions. For example, we could have the X matrix as follows\n",
    "$$X_{135\\times 3} = \\begin{bmatrix}\n",
    "x_1 & y_1 & z_1\\\\\n",
    "x_2 & y_2 & z_2\\\\\n",
    "\\cdots\\\\\n",
    "x_{135} & y_{135} & z_{135}\\\\\n",
    "\\end{bmatrix}$$\n",
    "2. `d`: This is the number of consecutive samples for each segment in the output.\n",
    "\n",
    "and returns the variable `out`, which arranges the vector into segments of size `d` and drops any incomplete final set of data. For instance, in our previous example we should have\n",
    "$$\\textit{out}_{4\\times 96} = \n",
    "\\begin{bmatrix}\n",
    "x_1 & y_1 & z_1 & x_2 & y_2 & z_2 & \\cdots & x_{32}& y_{32} & z_{32}\\\\\n",
    "\\cdots\\\\\n",
    "x_{97} & y_{97} & z_{97} & x_{98} & y_{98} & z_{98} & \\cdots & x_{128}& y_{128} & z_{128}\\\\\n",
    "\\end{bmatrix}$$\n",
    "Each row is a segment of 32 consecutive samples (each sample with their corresponding $x_i$, $y_i$, $z_i$ acceleration measurements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a88cf526dc813ef3977cefa42055bb7a",
     "grade": false,
     "grade_id": "cell-59fcdfbf4ea849ac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def quantize(X, d=32):\n",
    "    assert X.ndim == 2\n",
    "    assert X.shape[1] == 3\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    assert out.shape[1] == 3*d\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca4d4b2b7a1a41139b3c0d34de2db931",
     "grade": true,
     "grade_id": "cell-057a181831011d4f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_data = (np.arange(135*3).reshape(-1,3) ** 13) % 20\n",
    "some_q_data = quantize(some_data, d=32)\n",
    "assert np.array_equal(some_q_data, np.array([[ 0,  1, 12,  3,  4,  5, 16,  7,  8,  9,  0, 11, 12, 13,  4, 15,\n",
    "                                               16, 17,  8, 19,  0,  1, 12,  3,  4,  5, 16,  7,  8, 13,  4, 15,\n",
    "                                               0,  5,  0, 19, 12,  9,  4,  3,  4,  1,  8, 15,  8, 17, 12, 11,\n",
    "                                               4,  5,  8, 15,  0, 17,  4, 15,  8, 17, 16,  3,  0,  9,  8,  7,\n",
    "                                               0, 17,  8,  7,  0,  1, 16, 11,  8, 17,  8,  7,  8,  1, 16, 19,\n",
    "                                               12,  5,  4,  3, 12,  5,  8,  7, 16,  9, 12, 11,  8, 13,  8,  7],\n",
    "                                             [ 0,  5,  4,  3,  4,  5, 16, 11, 12,  1,  4, 11,  8,  9,  0,  7,\n",
    "                                               8, 17, 12, 11, 16,  5,  8,  7, 12, 17,  0, 11,  8, 17,  0,  7,\n",
    "                                               0,  5, 16, 19, 16,  1,  8, 11,  8, 13, 16, 11,  4, 13, 16, 11,\n",
    "                                               12, 13,  4, 15, 12, 13,  4, 15, 16,  9,  0,  3, 16, 17,  0,  3,\n",
    "                                               0,  5, 16, 15,  8,  1,  8, 19, 12,  5,  0,  7,  8, 13, 12, 19,\n",
    "                                               16,  9, 12, 15,  8,  1, 16, 19,  0, 13,  0, 19, 12, 17,  0,  3],\n",
    "                                             [ 0,  5, 16, 19, 16,  9,  0,  3,  0, 17,  8, 15, 16, 17, 12, 19,\n",
    "                                               8, 17,  8, 19, 16, 13,  0,  7, 12,  5, 12, 11, 16,  9,  4, 19,\n",
    "                                               0,  9,  0, 15, 12,  9,  8, 11,  8,  9,  8, 15, 16,  9,  4, 11,\n",
    "                                               4, 13,  8, 15, 12, 13,  4, 11,  0,  1,  4, 15,  0, 17,  8, 15,\n",
    "                                               0,  5,  0, 15, 12, 13, 16,  3,  8,  1,  4, 15,  0,  5,  0, 11,\n",
    "                                               12,  9,  8, 19,  8, 17, 12, 15, 12, 17,  0,  7,  4, 17,  0, 15],\n",
    "                                             [ 0, 17, 12,  7, 12, 17,  8, 15,  8, 13, 16, 11, 12,  1,  0,  7,\n",
    "                                               4,  9,  4, 19,  8,  5, 12,  3, 12,  1, 16,  3,  8,  9,  8, 11,\n",
    "                                               0, 17,  4, 19,  8,  5,  8, 15,  4, 17, 16,  7,  8,  5,  0, 15,\n",
    "                                               16,  9,  4,  3,  0,  1,  8, 15, 12, 13,  4, 19,  4,  1,  0,  3,\n",
    "                                               0,  1,  0, 11, 16,  9,  8, 15,  0, 13,  4, 15,  8, 17, 12,  3,\n",
    "                                               8,  1,  8, 11,  0,  9, 12, 15, 16,  9, 12,  7,  0,  9,  8, 19]]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(quantize, task_id=1)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_data_for_clustering = []\n",
    "for activity_idx, activity_txt_files in enumerate(train_txt_files):\n",
    "    for txt_df in activity_txt_files:\n",
    "        quantized_text = quantize(txt_df.values, d=d)\n",
    "        quantized_data_for_clustering.append(quantized_text)\n",
    "quantized_data_for_clustering = np.concatenate(quantized_data_for_clustering, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-learn's KMeans implementation, learn a K-Means clusterer. Write the function `train_kmeans_model` to get the training data `data` and `k` as arguments, and produce a SKLearn's KMeans object with `k` clusters that was trained on `data`.\n",
    "\n",
    "**Important**: You should use 12345 as the `random_state` variable for the sake of auto-grading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81576871d74b711042f6585c2835efc6",
     "grade": false,
     "grade_id": "cell-370f8a8bbaa20eca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_kmeans_model(data, k):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    return kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c177cb215dfec4a384cfa1af5bea7a4c",
     "grade": true,
     "grade_id": "cell-ce94176d91a9165c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "kmeans_model = train_kmeans_model(quantized_data_for_clustering, k)\n",
    "\n",
    "assert kmeans_model.n_clusters == k\n",
    "assert kmeans_model.random_state == 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `quantize` function you wrote before, write the new function `text2hist` that converts the data previously obtained from text files into a set of features using the K-Means model you have already trained.\n",
    "\n",
    "First, quantize the data. This should give you a matrix `quantized_data` with multiple rows which can then be fed to the K-Means clusterer. The output of the K-Means prediction `km_pred` has the same length as the number of rows in `quantized_data`, you should treat it as a set of samples. You should create a normalized count vector of length $k$. For normalization, consider that the prediction classes range between $0$ and $k-1$ in value. This normalized count vector would be your output. \n",
    "\n",
    "The inputs are:\n",
    "1. `X`: 1. `X`: a numpy array with the shape $(N, 3)$, where $N$ is the number of samples in a single recording. The columns represent the acceleration in each of the $x$, $y$, and $z$ directions. This is the same kind of input that was given to the `quantize` function.\n",
    "2. `kmeans_model`: This is a trained scikit-learn K-Means object that you could use for prediction.\n",
    "2. `d`: This is the vector quantization length.\n",
    "3. `k`: This is the number of clusters.\n",
    "\n",
    "The output should be a histogram `hist`; A numpy array with the shape of $(k,)$, and non-negative elements that should sum up to 1.\n",
    "\n",
    "**Hint**: Numpy functions like `np.bincount` or `np.histogram` maybe useful for histogram production if you know how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c127a1ce4f5c5e001fdb43a9cf89ae9",
     "grade": false,
     "grade_id": "cell-298e28e8eb3c77e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def text2hist(X, kmeans_model, d, k):\n",
    "    assert X.ndim == 2\n",
    "    assert X.shape[1] == 3\n",
    "    assert kmeans_model.cluster_centers_.shape == (k, 3*d)\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    assert hist.ndim == 1\n",
    "    assert hist.size == k\n",
    "    assert np.sum(hist).round(2) == 1.\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f470c102b6cd4c0391cd9abfbdebddbf",
     "grade": true,
     "grade_id": "cell-672859d0cc685228",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_data = (np.arange(135*3).reshape(-1,3) ** 13) % 20\n",
    "some_hist = text2hist(some_data, kmeans_model, d, k)\n",
    "assert some_hist[some_hist>0].size == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creating the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_maker(txt_files, kmeans_model, d, k):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for activity_idx, activity_txt_files in enumerate(txt_files):\n",
    "        for txt_df in activity_txt_files:\n",
    "            feature_vec = text2hist(txt_df.values, kmeans_model, d=d, k=k)\n",
    "            features.append(feature_vec.reshape(1,-1))\n",
    "            labels.append(activity_idx)\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = feature_maker(train_txt_files, kmeans_model, d, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Training the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-learn's implementation, train a Random Forest classifier. Write the function `train_classifier` to get the training data `train_features` and `train_labels` as arguments, and return a SKLearn's `RandomForestClassifier` object that was trained on `data`. Use 100 trees for building the random forest.\n",
    "\n",
    "**Important**: You should use 12345 as the `random_state` variable for the sake of auto-grading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "188b29239c86453fd4a6bebd559df2eb",
     "grade": false,
     "grade_id": "cell-a3c3a11007d17cf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(train_features, train_labels):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "476795c423e5b275d613976e277c3b47",
     "grade": true,
     "grade_id": "cell-f11a54fa5d18e21d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "classifier = train_classifier(train_features, train_labels)\n",
    "assert classifier.n_estimators == 100\n",
    "assert classifier.random_state == 12345\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = classifier.predict(train_features)\n",
    "print(f' Training accuracy: {np.mean(train_pred==train_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = feature_maker(test_txt_files, kmeans_model, d, k)\n",
    "test_pred = classifier.predict(test_features)\n",
    "print(f' Testing accuracy: {np.mean(test_pred==test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "fig, ax = plt.subplots(figsize=(8,8), dpi=100)\n",
    "plot_confusion_matrix(classifier, test_features, test_labels, \n",
    "                      display_labels=activity_labels, \n",
    "                      xticks_rotation = 'vertical', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function only combines what we already have done so far. It essentially takes the training and test data, along with the choice of `d` and `k` hyperparameters, trains a model, and then returns the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_txt_files, test_txt_files, d, k, plot_confusion_mat=False):\n",
    "    quantized_data_for_clustering = []\n",
    "    for activity_idx, activity_txt_files in enumerate(train_txt_files):\n",
    "        for txt_df in activity_txt_files:\n",
    "            quantized_text = quantize(txt_df.values, d=d)\n",
    "            quantized_data_for_clustering.append(quantized_text)\n",
    "    quantized_data_for_clustering = np.concatenate(quantized_data_for_clustering, axis=0)\n",
    "\n",
    "    kmeans_model = train_kmeans_model(quantized_data_for_clustering, k)\n",
    "\n",
    "    train_features, train_labels = feature_maker(train_txt_files, kmeans_model, d, k)\n",
    "\n",
    "    classifier = train_classifier(train_features, train_labels)\n",
    "\n",
    "    test_features, test_labels = feature_maker(test_txt_files, kmeans_model, d, k)\n",
    "    test_pred = classifier.predict(test_features)\n",
    "    test_acc = np.mean(test_pred==test_labels)\n",
    "    \n",
    "    if plot_confusion_mat:\n",
    "        fig, ax = plt.subplots(figsize=(8,8), dpi=100)\n",
    "        plot_confusion_matrix(classifier, test_features, test_labels, \n",
    "                              display_labels=activity_labels, \n",
    "                              xticks_rotation = 'vertical', ax=ax)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cross-Validation\n",
    "\n",
    "<font color='red'> <b>Attention: </b> </font> The followup quiz of this assignment will ask some questions about performance in cross-validation. Although you are not implementing this part, you may want to come back here to do some calculations to get answers for the questions in the quiz.\n",
    "\n",
    "<font color='red'><b> Warning: </b></font> Using the \"Validate\" button for this assignment may lead to a timeout, please do not use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Getting a Dry Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a tiny version of our dataset with at most 5 training items per class. Since running a 5 or 10-fold cross-validation would be extremely time consuming, we are running a 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cv_pairs(train_val_txt_files, cv_folds=3):\n",
    "    cross_val_pairs = []\n",
    "    for fold_idx in range(cv_folds):\n",
    "        train_cv_files = []\n",
    "        val_cv_files = []\n",
    "        for activity_idx, activity_txt_files in enumerate(train_val_txt_files):\n",
    "            train_cv_files.append([])\n",
    "            val_cv_files.append([])\n",
    "            for i, txt_df in enumerate(activity_txt_files):\n",
    "                if float(fold_idx+1)/cv_folds > float(i)/len(activity_txt_files) >= float(fold_idx)/cv_folds:\n",
    "                    val_cv_files[-1].append(txt_df)\n",
    "                else:\n",
    "                    train_cv_files[-1].append(txt_df)\n",
    "        cross_val_pairs.append((train_cv_files,val_cv_files))\n",
    "    return cross_val_pairs\n",
    "\n",
    "def perform_cross_validation(cross_val_pairs, k_list, d_list):\n",
    "    kd_acc = dict()\n",
    "    for k_candidate in k_list:\n",
    "        for d_candidate in d_list:\n",
    "            fold_accs = []\n",
    "            for train_txt_files, val_txt_files in cross_val_pairs:\n",
    "                print('.', end='')\n",
    "                fold_acc = train_and_evaluate(train_txt_files, val_txt_files, d_candidate, k_candidate)\n",
    "                fold_accs.append(fold_acc)\n",
    "            cv_acc = np.mean(fold_accs)\n",
    "\n",
    "            kd_acc[(k_candidate, d_candidate)] = cv_acc\n",
    "        print('')\n",
    "    return kd_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of k and d candidates for performing hyper-parameter optimization using Cross-Validation\n",
    "k_list = [50, 200, 500]\n",
    "d_list = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c190bf11dc0fe1b9d7f139bdac6b88da",
     "grade": true,
     "grade_id": "cell-8ea2a468de503648",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    train_val_txt_files_tiny = [x[:5] for x in train_val_txt_files]\n",
    "    test_txt_files_tiny = [x[:5] for x in test_txt_files]\n",
    "    cross_val_pairs_tiny = generate_cv_pairs(train_val_txt_files_tiny, cv_folds=3)\n",
    "    kd_acc = perform_cross_validation(cross_val_pairs_tiny, k_list=k_list, d_list=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    fig, ax = plt.subplots(figsize=(10,8), dpi=100)\n",
    "    for (k_,d_), acc_ in kd_acc.items():\n",
    "        ax.scatter([k_], [d_])\n",
    "        ax.annotate('%.1f'%(acc_*100.) + '%', (k_-int((max(k_list)-min(k_list))*0.022), d_*1.03))\n",
    "    ax.set_xlabel('Number of clusters')\n",
    "    ax.set_ylabel('Vector Quantization Length')\n",
    "    ax.set_yscale('symlog', base=2)\n",
    "    ax.set_yticks(d_list)\n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.ticklabel_format(axis='y', style='plain')\n",
    "    _ = ax.set_title('Cross-Validation Accuracy Values (*Dry Run)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    (best_k, best_d), best_cv_acc = max(kd_acc.items(), key=lambda tup_: tup_[1])\n",
    "    test_acc = train_and_evaluate(train_val_txt_files_tiny, test_txt_files_tiny, best_d, best_k, plot_confusion_mat=True)\n",
    "    print(f'Best Number of Clusters (*Dry Run): k={best_k}')\n",
    "    print(f'Best Quantization Length (*Dry Run): d={best_d}')\n",
    "    print(f'Tuned Test Accuracy (*Dry Run): {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Getting a More Serious Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform cross-validation with the full set of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following may take up to an hour, so please be patient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of k and d candidates for performing hyper-parameter optimization using Cross-Validation\n",
    "k_list = [50, 200, 500]\n",
    "d_list = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6f73ca8a67281d97c91cf1012dc594",
     "grade": true,
     "grade_id": "cell-44f38913016f6229",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    cross_val_pairs = generate_cv_pairs(train_val_txt_files, cv_folds=3)\n",
    "    kd_acc = perform_cross_validation(cross_val_pairs, k_list=k_list, d_list=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    fig, ax = plt.subplots(figsize=(10,8), dpi=100)\n",
    "    for (k_,d_), acc_ in kd_acc.items():\n",
    "        ax.scatter([k_], [d_])\n",
    "        ax.annotate('%.1f'%(acc_*100.) + '%', (k_-int((max(k_list)-min(k_list))*0.022), d_*1.03))\n",
    "    ax.set_xlabel('Number of clusters')\n",
    "    ax.set_ylabel('Vector Quantization Length')\n",
    "    ax.set_yscale('symlog', base=2)\n",
    "    ax.set_yticks(d_list)\n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.ticklabel_format(axis='y', style='plain')\n",
    "    _ = ax.set_title('Cross-Validation Accuracy Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    (best_k, best_d), best_cv_acc = max(kd_acc.items(), key=lambda tup_: tup_[1])\n",
    "    test_acc = train_and_evaluate(train_val_txt_files, test_txt_files, best_d, best_k, plot_confusion_mat=True)\n",
    "    print(f'Best Number of Clusters: k={best_k}')\n",
    "    print(f'Best Quantization Length: d={best_d}')\n",
    "    print(f'Tuned Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1423969a0ca8e9ce2ce84da241ac1ee",
     "grade": true,
     "grade_id": "cell-9618626c629fcad7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
